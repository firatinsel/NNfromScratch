{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"Neuron in neural network\n",
    "    \n",
    "    __init__(num_in,bias)\n",
    "        num_in => number of features to input to neuron (creates the number of weights on the neuron)\n",
    "        bias => True if a bias term should be included for w*x+b \n",
    "               False if no bias term should be included for w*x+b\n",
    "               default=True\n",
    "           \n",
    "\n",
    "    forward(x)\n",
    "        x => input to the neuron for w*x + b\n",
    "    \n",
    "    \n",
    "    backward(gradient_l)\n",
    "        gradient_l => the upstream gradient for backpropogation\n",
    "        gradient_x => the local gradient of input x\n",
    "        gradient_w => the local gradient of the neuron weights w\n",
    "        \n",
    "        returns local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,num_in,bias=True):\n",
    "        self.gradient_x = None\n",
    "        self.gradient_w = None\n",
    "        \n",
    "        if bias:\n",
    "            num_in = num_in + 1\n",
    "            \n",
    "        self.weights = np.random.uniform(-1,1,size=(num_in,1))\n",
    "        self.x = None\n",
    "        self.bias=bias\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.bias == True:\n",
    "            x = np.concatenate((x,[[1]]),axis=1)\n",
    "            \n",
    "        self.x = x\n",
    "        return np.matmul(x,self.weights)\n",
    "                                \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            self.gradient_x = self.weights.transpose()*gradient_l\n",
    "            self.gradient_w = self.x.transpose()*gradient_l\n",
    "            return self.gradient_x,self.gradient_w\n",
    "        else:\n",
    "            return None          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu:\n",
    "    \"\"\"ReLu Activation function\n",
    "    \n",
    "    forward(x)\n",
    "        x => the input to sigmoid function max(0,x)\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l => the upstream gradient for backpropogation\n",
    "        self.gradient => the local gradient of the ReLu activation function\n",
    "        \n",
    "        returns local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            if self.x > 0:\n",
    "                self.gradient = gradient_l\n",
    "                return gradient_l\n",
    "            else:\n",
    "                self.gradient = sys.float_info.epsilon\n",
    "                return np.array([[sys.float_info.epsilon]])   \n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid Activation function\n",
    "    \n",
    "    forward(x)\n",
    "        x => the input to sigmoid function f(x) = 1 / (1+exp(-x))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l => the upstream gradient for backpropogation\n",
    "        self.gradient => the local gradient of the Sigmoid activation function\n",
    "        \n",
    "        returns local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient=None\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            self.gradient = 1/(1+np.exp(-self.x))*(1-1/(1+np.exp(-self.x)))*gradient_l\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCE:\n",
    "    \"\"\"Binary Cross Entropy function\n",
    "    \n",
    "    forward(y_hat,y)\n",
    "        y_hat => the predicted classification probability 0-1\n",
    "        y => the training example actual label 0 or 1\n",
    "        binary cross entropy => -(y*ln(y_hat) + (1-y)*ln(1-y_hat))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l => the upstream gradient for backpropogation\n",
    "        self.gradient => the local gradient of the BCE \n",
    "        \n",
    "        returns local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.y = None\n",
    "        self.y_hat = None\n",
    "    \n",
    "    def forward(self,y_hat,y):\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        return -(y*np.log(max(sys.float_info.epsilon,y_hat-sys.float_info.epsilon)) + \\\n",
    "                 (1-y)*np.log(1-max(sys.float_info.epsilon,y_hat-sys.float_info.epsilon)))\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.y is not None and self.y_hat is not None:\n",
    "            self.gradient = -self.y*1/(max(sys.float_info.epsilon,self.y_hat)) + \\\n",
    "                              1/max(sys.float_info.epsilon,1-self.y_hat)*(1-self.y)\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CE:\n",
    "    \"\"\"Cross Entropy function\n",
    "    \n",
    "    forward(y_hat,y)\n",
    "        y_hat => the predicted classification probability 0-1\n",
    "        y => the training example actual label 0 or 1\n",
    "        binary cross entropy => -y_vector * ln(y_hat_vector)\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l => the upstream gradient for backpropogation\n",
    "        self.gradient => the local gradient of the CE \n",
    "        \n",
    "        returns local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.y = None\n",
    "        self.y_hat = None\n",
    "        \n",
    "    def forward(self,y_hat,y):\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        return -np.sum(y_hat*y)\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.y is not None and self.y_hat is not None:\n",
    "            self.gradient = -self.y*1/np.maximum(sys.float_info.epsilon,self.y_hat)\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    \"\"\"Softmax function\n",
    "    \n",
    "    forward(o)\n",
    "        o => the output of a fully connected layer for softmax normalization \n",
    "        output => exp(oi) / sum over all exp([o1,o2,...on])\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l => the upstream gradient for backpropogation\n",
    "        self.gradient => the local gradient of the softmax\n",
    "        \n",
    "        returns local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,n_classes=3):\n",
    "        self.gradient = None\n",
    "        self.o = None\n",
    "        self.p = None\n",
    "        self.n_classes = 3\n",
    "\n",
    "    def forward(self,o):\n",
    "        self.o = o\n",
    "        self.p = np.exp(o)/np.sum(np.exp(o))\n",
    "        return self.p\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if self.o is not None:\n",
    "            # fix size...\n",
    "            if gradient_l.shape[1] > self.o.shape[1]:\n",
    "                gradient_l = gradient_l[:,:self.o.shape[1]]\n",
    "            \n",
    "            p1 = self.p*(1-self.p)\n",
    "            self.gradient = -1*np.matmul(self.p.transpose(),self.p)\n",
    "            np.fill_diagonal(self.gradient,p1)\n",
    "            self.gradient = np.expand_dims(np.sum(self.gradient*gradient_l,1),0)\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"softmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_layer:\n",
    "    \"\"\"Neural Network layer\n",
    "    \n",
    "    __init__(input_,output_,bias=True)\n",
    "        input_ => the number of features in the training example\n",
    "        output_ => the number of neurons to use in the neural network layer\n",
    "        bias => True if a bias term should be included for w*x+b \n",
    "               False if no bias term should be included for w*x+b\n",
    "               default=True\n",
    "    \n",
    "    forward(x):\n",
    "        x => input to the neuron for W*x + B\n",
    "        \n",
    "    backward(gradient_l):\n",
    "        gradient_l => the upstream gradient for backpropogation\n",
    "        gradients has gradient_x and gradient_w for each neuron in the neural network layer\n",
    "        \n",
    "        returns local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,input_= 1,output_= 1,bias=True):\n",
    "        self.output_size = output_\n",
    "        self.input_size = input_\n",
    "        self.neurons = [Neuron(input_,bias) for _ in range(output_)]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        outputs = [neuron.forward(x) for neuron in self.neurons]\n",
    "        return np.concatenate(outputs,1)\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if gradient_l is None:\n",
    "            return None\n",
    "        \n",
    "        weight_gradients = []\n",
    "        input_gradient = 0\n",
    "        gradient_l = gradient_l.squeeze(0)  \n",
    "        \n",
    "        for i,neuron in enumerate(self.neurons):\n",
    "            temp_gradient = neuron.backward(gradient_l[i])\n",
    "            weight_gradients.append(temp_gradient[1])\n",
    "            input_gradient = input_gradient + temp_gradient[0]\n",
    "            \n",
    "        gradients = (input_gradient,weight_gradients)\n",
    "                    \n",
    "        return gradients\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"neuron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class action_layer:\n",
    "    \"\"\"Activation layer\n",
    "    \n",
    "    __init__(output_,a_type=\"ReLu\")\n",
    "        output_ => the number of activation units to use in the neural network layer\n",
    "        a_type => the type of activation unit you want to use: \"ReLu\" or \"Sigmoid\"\n",
    "    \n",
    "    forward(x):\n",
    "        x => input to the activation layer\n",
    "        return the activation evaluation\n",
    "        \n",
    "    backward(gradient_l):\n",
    "        gradient_l => the upstream gradient for backpropogation\n",
    "        gradients has the local gradients for each activation unit\n",
    "        \n",
    "        returns local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,output_= 1,a_type=\"ReLu\"):\n",
    "        \n",
    "        self.output_size = output_\n",
    "        \n",
    "        if a_type == \"ReLu\":\n",
    "            self.activations = [ReLu() for _ in range(output_)]\n",
    "        if a_type == \"Sigmoid\":\n",
    "            self.activations = [Sigmoid() for _ in range(output_)]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output = []\n",
    "        x = x.squeeze(0)\n",
    "        for i,activation in enumerate(self.activations):\n",
    "            output.append(activation.forward(x[i]))\n",
    "        return np.expand_dims(output,0)\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if gradient_l is None:\n",
    "            return None\n",
    "        \n",
    "        gradients = []\n",
    "        gradient_l = gradient_l.squeeze(0)     \n",
    "                \n",
    "        \n",
    "        for i,activation in enumerate(self.activations):\n",
    "            gradients.append(activation.backward(gradient_l[i]))\n",
    "            \n",
    "        return np.expand_dims(gradients,0)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"activation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    \"\"\"Neural Network\n",
    "    \n",
    "    __init__(model=[])\n",
    "        model => a list [] of all the neural network layer objects and activation layer objects. The forward and backward execution\n",
    "                is sequential in regards to the order of the list passed in to model\n",
    "    \n",
    "    forward(x)\n",
    "        x => the input training example to the neural network which will pass through all the layers of neural network and activation\n",
    "            layers to classify or regress\n",
    "    \n",
    "    backward(gradient_l)\n",
    "        gradient_l => the upstream gradient from the loss function to the neural network\n",
    "\n",
    "        returns all the gradients calculated throughout all the layers  \n",
    "        \n",
    "    model_weights()\n",
    "        returns a list with all the model weights for each neural network layer with neurons\n",
    "    \"\"\"\n",
    "    def __init__(self,model=[]):\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for obj in self.model:\n",
    "            x = obj.forward(x)    \n",
    "        return x\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        self.gradients = []\n",
    "        for obj in self.model[::-1]:\n",
    "            if str(obj) == \"activation\":\n",
    "                tup = (\"activation\",obj.backward(gradient_l))\n",
    "                self.gradients.append(tup)\n",
    "                gradient_l = tup[1]\n",
    "            \n",
    "            elif str(obj) == \"neuron\":\n",
    "                tup = (\"neuron\",obj.backward(gradient_l))\n",
    "                self.gradients.append(tup)\n",
    "                \n",
    "                if gradient_l is None:\n",
    "                    gradient_l = None\n",
    "                    continue\n",
    "                \n",
    "                gradient_l = tup[1][0]\n",
    "                \n",
    "            elif str(obj) == \"softmax\":\n",
    "                tup = (\"softmax\",obj.backward(gradient_l))\n",
    "                self.gradients.append(tup)\n",
    "                gradient_l = tup[1]\n",
    "\n",
    "        self.gradients = self.gradients[::-1]\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_weights(self):\n",
    "        weights = []\n",
    "        layer_num = 0\n",
    "        for obj in self.model:\n",
    "            if str(obj) == 'neuron':\n",
    "                layer_weights = []\n",
    "                for neuron in obj.neurons:\n",
    "                    layer_weights.append(neuron.weights)\n",
    "                weights.append(('layer'+str(layer_num),layer_weights))\n",
    "                layer_num += 1\n",
    "        return weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer:\n",
    "    \n",
    "    def __init__(self,model = None,alpha=0.05):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def step(self):\n",
    "        if model is not None:\n",
    "            for obj,grad in zip(self.model.model,self.model.gradients):\n",
    "                if grad[0] == 'neuron':\n",
    "                    for n,grad_update in zip(obj.neurons,grad[1][1]):\n",
    "                        n.weights = n.weights - self.alpha*grad_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Classification Dataset (use the breast cancer dataset)\n",
    "* Download and use the breast cancer dataset with sklearn\n",
    "* standard normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data']\n",
    "X = X - np.mean(X,0)\n",
    "X = X / np.std(X,0)\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to do logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = [[0.20373497]]\n",
      "epoch 1: loss = [[0.11785385]]\n",
      "epoch 2: loss = [[0.19035256]]\n",
      "epoch 3: loss = [[0.12353642]]\n",
      "epoch 4: loss = [[0.13342761]]\n",
      "epoch 5: loss = [[0.14967978]]\n",
      "epoch 6: loss = [[0.18374776]]\n",
      "epoch 7: loss = [[0.11885001]]\n",
      "epoch 8: loss = [[0.12690265]]\n",
      "epoch 9: loss = [[0.13469437]]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "print_every = 1\n",
    "for i in range(10):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = neuron.forward(x)\n",
    "        a = activation.forward(output)\n",
    "        \n",
    "        loss = loss_fn.forward(a,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_grad = loss_fn.backward()\n",
    "        a_grad = activation.backward(loss_grad)\n",
    "        _,weight_grad = neuron.backward(a_grad)\n",
    "\n",
    "        # update model\n",
    "        neuron.weights = neuron.weights - alpha*weight_grad\n",
    "        \n",
    "    if i % print_every == 0:\n",
    "        print(\"epoch {}: loss = {}\".format(i,np.mean(mean_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = neuron.forward(x)\n",
    "    a = activation.forward(output)\n",
    "    \n",
    "    y_predictions.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.round(np.array(y_predictions).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9806678383128296\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == Y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [neural_layer(30,10),softmax(10),neural_layer(10,5),action_layer(5,\"Sigmoid\"),neural_layer(5,1),action_layer(a_type='Sigmoid')]\n",
    "nn = neural_network(model)\n",
    "loss_fn = BCE()\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "optimize = optimizer(nn,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 0.5955479917240056\n",
      "epoch 5: loss = 0.0997896153972692\n",
      "epoch 10: loss = 0.05320168245043995\n",
      "epoch 15: loss = 0.04219783650138419\n",
      "epoch 20: loss = 0.032992257903424395\n",
      "epoch 25: loss = 0.024668849147828503\n",
      "epoch 30: loss = 0.016197172874834423\n",
      "epoch 35: loss = 0.009157234290145878\n",
      "epoch 40: loss = 0.005941394781100225\n",
      "epoch 45: loss = 0.004244364711458586\n",
      "epoch 50: loss = 0.003203953556643791\n",
      "epoch 55: loss = 0.002573611828744099\n",
      "epoch 60: loss = 0.0021536395294654617\n",
      "epoch 65: loss = 0.0018523177609740223\n",
      "epoch 70: loss = 0.0016247326987888606\n",
      "epoch 75: loss = 0.0014464093466702917\n",
      "epoch 80: loss = 0.0013027794269864375\n",
      "epoch 85: loss = 0.0011845713213689487\n",
      "epoch 90: loss = 0.0010855762807186493\n",
      "epoch 95: loss = 0.0010014679258710658\n"
     ]
    }
   ],
   "source": [
    "print_every = 5\n",
    "for i in range(100):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = nn.forward(x)\n",
    "        loss = loss_fn.forward(output,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_back = loss_fn.backward()\n",
    "        nn_back = nn.backward(loss_back)\n",
    "\n",
    "        # update weights based on gradient\n",
    "        optimize.step()\n",
    "    if i % print_every == 0:\n",
    "        print(\"epoch {}: loss = {}\".format(i,np.mean(mean_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = nn.forward(x)\n",
    "    \n",
    "    y_predictions.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(np.array(y_predictions).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == Y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Classification Dataset (Iris)\n",
    "* Download and use the breast cancer dataset with sklearn\n",
    "* standard normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data  # we only take the first two features.\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - np.mean(X,0)\n",
    "X = X / np.std(X,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = np.max(Y)+1\n",
    "Y = np.eye(n_values)[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [neural_layer(4,10),action_layer(10,\"ReLu\"),neural_layer(10,5),softmax(5),neural_layer(5,3),softmax(3)]\n",
    "nn = neural_network(model)\n",
    "loss_fn = CE()\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "optimize = optimizer(nn,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = -0.5533020999299668\n",
      "epoch 5: loss = -0.8404591386353584\n",
      "epoch 10: loss = -0.9455900857715404\n",
      "epoch 15: loss = -0.9649839499466729\n",
      "epoch 20: loss = -0.9654939985451734\n",
      "epoch 25: loss = -0.9660074587415439\n",
      "epoch 30: loss = -0.953437450756918\n",
      "epoch 35: loss = -0.955249718008353\n",
      "epoch 40: loss = -0.9703568537511645\n",
      "epoch 45: loss = -0.9684541677678161\n"
     ]
    }
   ],
   "source": [
    "print_every = 5\n",
    "for i in range(50):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = nn.forward(x)\n",
    "        loss = loss_fn.forward(output,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_back = loss_fn.backward()\n",
    "        nn_back = nn.backward(loss_back)\n",
    "\n",
    "        # update weights based on gradient\n",
    "        optimize.step()\n",
    "        \n",
    "    if i % print_every == 0:\n",
    "        print(\"epoch {}: loss = {}\".format(i,np.mean(mean_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = nn.forward(x)\n",
    "    \n",
    "    y_predictions.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(np.array(y_predictions).squeeze(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(np.argmax(Y,1) == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
